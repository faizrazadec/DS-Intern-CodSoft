{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73a55422",
   "metadata": {},
   "source": [
    "# Iris Flower Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6aac6",
   "metadata": {},
   "source": [
    "   ### Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa93cf",
   "metadata": {},
   "source": [
    "First: Import the pandas library to read the file, to see the file, use .head() method for first rows of the dataframe, cause maybe your data will be soo much large, so it's a good practice to print just 5 rows instead of all dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29b8ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "808ed963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/faizr/Interns/DS@CodSoft/Task -3/archive_11/IRIS.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731ec0a",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0848e2",
   "metadata": {},
   "source": [
    "For the cleaning of data, there are many techinques, everyone use the algos to do so!!\n",
    "The Best Data Cleaning Techniques for Preparing Your Data\n",
    "- 1 Remove unnecessary values\n",
    "- 2 Remove duplicate data.\n",
    "- 3 Convert data types.\n",
    "- 4 Search for missing values.\n",
    "- 5 Use a clear format.\n",
    "- 6 Remove unwanted outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54934ee3",
   "metadata": {},
   "source": [
    "- 1 Removing Unnecessary Values: Unnecessary values is just making the size of data, it effects nothings or affect less on the data, we usually remove unnecessary features form the dataframe, because it make the high accuracy of the model. *but luckily here we don't have any unnecessary values or features!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f10ff7",
   "metadata": {},
   "source": [
    "- 2 Removing Duplicates: Duplicates also just make the data size big, so we remove them, well, if we have very less data, and we want to train a model, it'll give us very bad accuracy, so there, we make duplicates to we can get good accuracy because as model requires a large data for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2257c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the duplicates\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd70eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the duplicates\n",
    "data.drop_duplicates(keep = False, inplace = True)\n",
    "#confirming, if the duplicates are removed succussfully\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248c975",
   "metadata": {},
   "source": [
    "- 3 Convert data types: We have to convert the data types of the values to their defaults, e.g we have a column of Age, where the values are in '2,3,4,5', it's int by default, but if it's str, then there's a problem, in future we might have to process the Age column, but it'll show us error as we can't process on str!(math functions)\n",
    "- That's why, convert the data type to their defaults!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1675172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking datatypes\n",
    "data.dtypes  #Everything is good!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43160f",
   "metadata": {},
   "source": [
    "- 4 Search for missing values: In the data, there might be possible, we're missing some values, like someone forget to put his/her Age, and in the database, it's empty\n",
    "- To get better accuracy, and make our model to fit properly, we have to replace the missing values with their relevent ones, e.g if we're missing the Age, we can replace the Age with the most common value (which is present in many datasets) etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the NAN (not a number)\n",
    "data.isna().sum()   # all good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7efe45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the NUll \n",
    "data.isnull().sum()   #all good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd70af1",
   "metadata": {},
   "source": [
    "- 5 Use a clear format: We have to make the one format in all the data so we can get the good accuracy\n",
    "- from one format i mean, just suppose we're asked to fill a box, in which we have to insert out Age, suppose the age is 12, some will type like 12 years, some will like 12.4, some will like 12 years 4 months, \n",
    "- so there's many format for just a single thing, we have to set one format, it's also a part of cleaning the data. \n",
    "- *but luckily here, we don't have any*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c08c6a3",
   "metadata": {},
   "source": [
    "- 6 Remove unwanted outliers: Outliers means that out dataset vary from others, it can be very large or very small, it effects the statistical analysis, Machine Learing Models, and many more.\n",
    "    that's why we remove the outliers, it also helps us for getting the good accuracy of the ML model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f4d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the outliers with visulizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (20, 5))\n",
    "ax[0].hist(data.drop('species', axis = 1), bins=4)\n",
    "sns.boxplot(data, ax=ax[1])\n",
    "plt.show()  #we see  that there's an outlier in the column \"sepal_width\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ecd96",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae90823",
   "metadata": {},
   "source": [
    "- Now we'll split the data into labels (features) and target, it'll help us to train the model in features so it can predict the target easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f60aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop('species', axis = 1)  #features\n",
    "y = data['species']   #target\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d606e",
   "metadata": {},
   "source": [
    "- This code will remove the outliers, that will help the model to predict easy with more accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7c6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data, (free from outliers)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_columns = x.select_dtypes(include=['float64', 'int64']).columns  #this line is restricting the data that\n",
    "#it must be int and float, not the str will be allowed in standard scaler\n",
    "\n",
    "normalizing = ColumnTransformer([\n",
    "    ('Normalizing', StandardScaler(), numeric_columns)\n",
    "], remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c942944",
   "metadata": {},
   "source": [
    "- Now, we already Normalized the data, we'll split the data in traing set and the testing set, we'll train the model on training set and then predict it's accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb28a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\n",
    "\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427c33f",
   "metadata": {},
   "source": [
    "- we're gonna use the KNN model, you can use with your mindset, but you have to look what kind of problem we're facing, use the model according to it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b874c403",
   "metadata": {},
   "source": [
    "### Importing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4421853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324ff54",
   "metadata": {},
   "source": [
    "- As everything is set now, we'll make the Pipeline, as Pileline has a lot more beneficial to us, it'll help us to write the productive code, with no headache of reverse method as we did in during the training of model! Pipeline do that all by itself! Thanks to PipelineðŸ˜™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('normalizing', normalizing),\n",
    "    ('model', model)\n",
    "])\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2408552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the data in the pipeline\n",
    "pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0415a1",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c4633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "y_predict = pipe.predict(x_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6114d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(x_test, y_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85819a9",
   "metadata": {},
   "source": [
    "- Classification Report provides us a large amount of informantion, For each class in a classification difficulty, scikit-learn gives an extensive overview of classification performance indicators. It covers measurements like support, recall, precision, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score\n",
    "print(accuracy_score(y_predict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61910ae6",
   "metadata": {},
   "source": [
    "### Confucion Matrix with heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a5423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "conmat = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "sns.heatmap(conmat, annot=True, fmt=\".1f\", linewidth=1, cmap=\"crest\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaef4d0",
   "metadata": {},
   "source": [
    "### Exporting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f160cb1",
   "metadata": {},
   "source": [
    "- we're here saving the file using the pickle, so we can use it in Production side, but we just saved it so we can see the deployment\n",
    "- is the model is properlly trained or not\n",
    "- so let's see!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0face65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(pipe,open(\"Deployment of Iris Flower Classification using Pipeline.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d6ca5a",
   "metadata": {},
   "source": [
    "# Let's go the the Deployment side, so see the Prediction in real time!ðŸ˜‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
